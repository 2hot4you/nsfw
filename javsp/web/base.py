"""ç½‘ç»œè¯·æ±‚çš„ç»Ÿä¸€æ¥å£"""
import os
import sys
import time
import shutil
import logging
import requests
import contextlib
import cloudscraper
import lxml.html
from tqdm import tqdm
from lxml import etree
from lxml.html.clean import Cleaner
from requests.models import Response


from javsp.config import Cfg
from javsp.web.exceptions import *


__all__ = ['Request', 'get_html', 'post_html', 'request_get', 'resp2html', 'is_connectable', 'download', 'get_resp_text', 'read_proxy']


headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'}

logger = logging.getLogger(__name__)
# åˆ é™¤jsè„šæœ¬ç›¸å…³çš„tagï¼Œé¿å…ç½‘é¡µæ£€æµ‹åˆ°æ²¡æœ‰jsè¿è¡Œç¯å¢ƒæ—¶å¼ºè¡Œè·³è½¬ï¼Œå½±å“è°ƒè¯•
cleaner = Cleaner(kill_tags=['script', 'noscript'])

def read_proxy():
    if Cfg().network.proxy_server is None:
        return {}
    else:
        proxy = str(Cfg().network.proxy_server)
        return {'http': proxy, 'https': proxy}

# ä¸ç½‘ç»œè¯·æ±‚ç›¸å…³çš„åŠŸèƒ½æ±‡æ€»åˆ°ä¸€ä¸ªæ¨¡å—ä¸­ä»¥æ–¹ä¾¿å¤„ç†ï¼Œä½†æ˜¯ä¸åŒç«™ç‚¹çš„æŠ“å–å™¨åˆæœ‰è‡ªå·±çš„éœ€æ±‚ï¼ˆé’ˆå¯¹ä¸åŒç½‘ç«™
# éœ€è¦ä½¿ç”¨ä¸åŒçš„UAã€è¯­è¨€ç­‰ï¼‰ã€‚æ¯æ¬¡éƒ½ä¼ é€’å‚æ•°å¾ˆéº»çƒ¦ï¼Œè€Œä¸”ä¼šé¢ä¸´å‡½æ•°å‚æ•°è¶ŠåŠ è¶Šå¤šçš„é—®é¢˜ã€‚å› æ­¤æ·»åŠ è¿™ä¸ª
# å¤„ç†ç½‘ç»œè¯·æ±‚çš„ç±»ï¼Œå®ƒå¸¦æœ‰é»˜è®¤çš„å±æ€§ï¼Œä½†æ˜¯ä¹Ÿå¯ä»¥åœ¨å„ä¸ªæŠ“å–å™¨æ¨¡å—é‡Œè¿›è¡Œè¿›è¡Œå®šåˆ¶
class Request():
    """ä½œä¸ºç½‘ç»œè¯·æ±‚å‡ºå£å¹¶æ”¯æŒå„ä¸ªæ¨¡å—å®šåˆ¶åŠŸèƒ½"""
    def __init__(self, use_scraper=False) -> None:
        # å¿…é¡»ä½¿ç”¨copy()ï¼Œå¦åˆ™å„ä¸ªæ¨¡å—å¯¹headersçš„ä¿®æ”¹éƒ½å°†ä¼šæŒ‡å‘æœ¬æ¨¡å—ä¸­å®šä¹‰çš„headerså˜é‡ï¼Œå¯¼è‡´åªæœ‰æœ€åä¸€ä¸ªå¯¹headersçš„ä¿®æ”¹ç”Ÿæ•ˆ
        self.headers = headers.copy()
        self.cookies = {}

        self.proxies = read_proxy()
        self.timeout = Cfg().network.timeout.total_seconds()
        if not use_scraper:
            self.scraper = None
            self.__get = requests.get
            self.__post = requests.post
            self.__head = requests.head
        else:
            self.scraper = cloudscraper.create_scraper()
            self.__get = self._scraper_monitor(self.scraper.get)
            self.__post = self._scraper_monitor(self.scraper.post)
            self.__head = self._scraper_monitor(self.scraper.head)

    def _scraper_monitor(self, func):
        """ç›‘æ§cloudscraperçš„å·¥ä½œçŠ¶æ€ï¼Œé‡åˆ°ä¸æ”¯æŒçš„Challengeæ—¶å°è¯•é€€å›å¸¸è§„çš„requestsè¯·æ±‚"""
        def wrapper(*args, **kw):
            try:
                return func(*args, **kw)
            except Exception as e:
                logger.debug(f"æ— æ³•é€šè¿‡CloudFlareæ£€æµ‹: '{e}', å°è¯•é€€å›å¸¸è§„çš„requestsè¯·æ±‚")
                if func == self.scraper.get:
                    return requests.get(*args, **kw)
                else:
                    return requests.post(*args, **kw)
        return wrapper

    def get(self, url, delay_raise=False):
        r = self.__get(url,
                      headers=self.headers,
                      proxies=self.proxies,
                      cookies=self.cookies,
                      timeout=self.timeout)
        if not delay_raise:
            r.raise_for_status()
        return r

    def post(self, url, data, delay_raise=False):
        r = self.__post(url,
                      data=data,
                      headers=self.headers,
                      proxies=self.proxies,
                      cookies=self.cookies,
                      timeout=self.timeout)
        if not delay_raise:
            r.raise_for_status()
        return r

    def head(self, url, delay_raise=True):
        r = self.__head(url,
                      headers=self.headers,
                      proxies=self.proxies,
                      cookies=self.cookies,
                      timeout=self.timeout)
        if not delay_raise:
            r.raise_for_status()
        return r

    def get_html(self, url):
        r = self.get(url)
        html = resp2html(r)
        return html


class DownloadProgressBar(tqdm):
    def update_to(self, b=1, bsize=1, tsize=None):
        if tsize is not None:
            self.total = tsize
        self.update(b * bsize - self.n)


def request_get(url, cookies={}, timeout=None, delay_raise=False):
    """è·å–æŒ‡å®šurlçš„åŸå§‹è¯·æ±‚"""
    if timeout is None:
        timeout = Cfg().network.timeout.seconds
    
    logger.debug(f"ğŸŒ å‘é€è¯·æ±‚: GET {url}")
    start_time = time.time()
    r = requests.get(url, headers=headers, proxies=read_proxy(), cookies=cookies, timeout=timeout)
    elapsed = time.time() - start_time
    
    # è®°å½•å“åº”ä¿¡æ¯
    status_emoji = "âœ…" if r.status_code < 400 else "âŒ"
    logger.debug(f"{status_emoji} å“åº”çŠ¶æ€: {r.status_code} ({round(elapsed*1000)}ms) - {url}")
    
    if not delay_raise:
        if r.status_code == 403 and b'>Just a moment...<' in r.content:
            logger.error(f"ğŸš« CloudFlare é˜»æ­¢: {url}")
            raise SiteBlocked(f"403 Forbidden: æ— æ³•é€šè¿‡CloudFlareæ£€æµ‹: {url}")
        else:
            r.raise_for_status()
    return r


def request_post(url, data, cookies={}, timeout=None, delay_raise=False):
    """å‘æŒ‡å®šurlå‘é€postè¯·æ±‚"""
    if timeout is None:
        timeout = Cfg().network.timeout.seconds
    
    logger.debug(f"ğŸŒ å‘é€è¯·æ±‚: POST {url}")
    start_time = time.time()
    r = requests.post(url, data=data, headers=headers, proxies=read_proxy(), cookies=cookies, timeout=timeout)
    elapsed = time.time() - start_time
    
    # è®°å½•å“åº”ä¿¡æ¯
    status_emoji = "âœ…" if r.status_code < 400 else "âŒ"
    logger.debug(f"{status_emoji} å“åº”çŠ¶æ€: {r.status_code} ({round(elapsed*1000)}ms) - {url}")
    
    if not delay_raise:
        r.raise_for_status()
    return r


def get_resp_text(resp: Response, encoding=None):
    """æå–Responseçš„æ–‡æœ¬"""
    if encoding:
        resp.encoding = encoding
    else:
        resp.encoding = resp.apparent_encoding
    return resp.text


def get_html(url, encoding='utf-8'):
    """ä½¿ç”¨getæ–¹æ³•è®¿é—®æŒ‡å®šç½‘é¡µå¹¶è¿”å›ç»lxmlè§£æåçš„document"""
    resp = request_get(url)
    text = get_resp_text(resp, encoding=encoding)
    html = lxml.html.fromstring(text)
    html.make_links_absolute(url, resolve_base_href=True)
    # æ¸…ç†åŠŸèƒ½ä»…åº”åœ¨éœ€è¦çš„æ—¶å€™ç”¨æ¥è°ƒè¯•ç½‘é¡µï¼ˆå¦‚prestigeï¼‰ï¼Œå¦åˆ™å¯èƒ½åè¿‡æ¥å½±å“è°ƒè¯•ï¼ˆå¦‚JavBusï¼‰
    # html = cleaner.clean_html(html)
    if hasattr(sys, 'javsp_debug_mode'):
        lxml.html.open_in_browser(html, encoding=encoding)  # for develop and debug
    return html


def resp2html(resp, encoding='utf-8') -> lxml.html.HtmlComment:
    """å°†requestè¿”å›çš„responseè½¬æ¢ä¸ºç»lxmlè§£æåçš„document"""
    text = get_resp_text(resp, encoding=encoding)
    html = lxml.html.fromstring(text)
    html.make_links_absolute(resp.url, resolve_base_href=True)
    # html = cleaner.clean_html(html)
    if hasattr(sys, 'javsp_debug_mode'):
        lxml.html.open_in_browser(html, encoding=encoding)  # for develop and debug
    return html


def post_html(url, data, encoding='utf-8', cookies={}):
    """ä½¿ç”¨postæ–¹æ³•è®¿é—®æŒ‡å®šç½‘é¡µå¹¶è¿”å›ç»lxmlè§£æåçš„document"""
    resp = request_post(url, data, cookies=cookies)
    text = get_resp_text(resp, encoding=encoding)
    html = lxml.html.fromstring(text)
    # jav321æä¾›ed2kå½¢å¼çš„èµ„æºé“¾æ¥ï¼Œå…¶ä¸­çš„éASCIIå­—ç¬¦å¯èƒ½å¯¼è‡´è½¬æ¢å¤±è´¥ï¼Œå› æ­¤è¦å…ˆè¿›è¡Œå¤„ç†
    ed2k_tags = html.xpath("//a[starts-with(@href,'ed2k://')]")
    for tag in ed2k_tags:
        tag.attrib['ed2k'], tag.attrib['href'] = tag.attrib['href'], ''
    html.make_links_absolute(url, resolve_base_href=True)
    for tag in ed2k_tags:
        tag.attrib['href'] = tag.attrib['ed2k']
        tag.attrib.pop('ed2k')
    # html = cleaner.clean_html(html)
    # lxml.html.open_in_browser(html, encoding=encoding)  # for develop and debug
    return html


def dump_xpath_node(node, filename=None):
    """å°†xpathèŠ‚ç‚¹dumpåˆ°æ–‡ä»¶"""
    if not filename:
        filename = node.tag + '.html'
    with open(filename, 'wt', encoding='utf-8') as f:
        content = etree.tostring(node, pretty_print=True).decode('utf-8')
        f.write(content)


def is_connectable(url, timeout=3):
    """æµ‹è¯•ä¸æŒ‡å®šurlçš„è¿æ¥"""
    try:
        r = requests.get(url, headers=headers, timeout=timeout)
        return True
    except requests.exceptions.RequestException as e:
        logger.debug(f"Not connectable: {url}\n" + repr(e))
        return False


def urlretrieve(url, filename=None, reporthook=None, headers=None):
    if "arzon" in url:
        headers["Referer"] = "https://www.arzon.jp/"
    """ä½¿ç”¨requestså®ç°urlretrieve"""
    # https://blog.csdn.net/qq_38282706/article/details/80253447
    with contextlib.closing(requests.get(url, headers=headers,
                                         proxies=read_proxy(), stream=True)) as r:
        header = r.headers
        with open(filename, 'wb+') as fp:
            bs = 1024
            size = -1
            blocknum = 0
            if "content-length" in header:
                size = int(header["Content-Length"])    # æ–‡ä»¶æ€»å¤§å°ï¼ˆç†è®ºå€¼ï¼‰
            if reporthook:                              # å†™å…¥å‰è¿è¡Œä¸€æ¬¡å›è°ƒå‡½æ•°
                reporthook(blocknum, bs, size)
            for chunk in r.iter_content(chunk_size=1024):
                if chunk:
                    fp.write(chunk)
                    fp.flush()
                    blocknum += 1
                    if reporthook:
                        reporthook(blocknum, bs, size)  # æ¯å†™å…¥ä¸€æ¬¡è¿è¡Œä¸€æ¬¡å›è°ƒå‡½æ•°


def download(url, output_path, desc=None):
    """ä¸‹è½½æŒ‡å®šurlçš„èµ„æº"""
    # æ”¯æŒ"ä¸‹è½½"æœ¬åœ°èµ„æºï¼Œä»¥ä¾›fc2fançš„æœ¬åœ°é•œåƒæ‰€ä½¿ç”¨
    if not url.startswith('http'):
        start_time = time.time()
        logger.debug(f"ğŸ“‹ å¤åˆ¶æœ¬åœ°æ–‡ä»¶: {url} -> {output_path}")
        shutil.copyfile(url, output_path)
        filesize = os.path.getsize(url)
        elapsed = time.time() - start_time
        info = {'total': filesize, 'elapsed': elapsed, 'rate': filesize/elapsed}
        logger.debug(f"âœ… å¤åˆ¶å®Œæˆ: {filesize/1024/1024:.2f}MB, è€—æ—¶: {elapsed:.2f}ç§’")
        return info
    if not desc:
        desc = url.split('/')[-1]
    
    logger.debug(f"â¬‡ï¸ å¼€å§‹ä¸‹è½½: {desc} -> {output_path}")
    referrer = headers.copy()
    referrer['referer'] = url[:url.find('/', 8)+1]  # æå–base_urléƒ¨åˆ†
    with DownloadProgressBar(unit='B', unit_scale=True,
                             miniters=1, desc=desc, leave=False) as t:
        urlretrieve(url, filename=output_path, reporthook=t.update_to, headers=referrer)
        info = {k: t.format_dict[k] for k in ('total', 'elapsed', 'rate')}
        size_mb = info['total'] / 1024 / 1024
        rate_mb = info['rate'] / 1024 / 1024
        logger.debug(f"âœ… ä¸‹è½½å®Œæˆ: {desc} ({size_mb:.2f}MB), é€Ÿåº¦: {rate_mb:.2f}MB/s, è€—æ—¶: {info['elapsed']:.2f}ç§’")
        return info


def open_in_chrome(url, new=0, autoraise=True):
    """ä½¿ç”¨æŒ‡å®šçš„Chrome Profileæ‰“å¼€urlï¼Œä¾¿äºè°ƒè¯•"""
    import subprocess
    chrome = R'C:\Program Files\Google\Chrome\Application\chrome.exe'
    subprocess.run(f'"{chrome}" --profile-directory="Profile 2" {url}', shell=True)

import webbrowser
webbrowser.open = open_in_chrome


if __name__ == "__main__":
    import pretty_errors
    pretty_errors.configure(display_link=True)
    download('https://www.javbus.com/pics/cover/6n54_b.jpg', 'cover.jpg')
